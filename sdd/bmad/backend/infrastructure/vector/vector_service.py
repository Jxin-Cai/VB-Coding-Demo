"""
å‘é‡åŒ–æœåŠ¡æ¨¡å—
è´Ÿè´£è¡¨ç»“æ„çš„å‘é‡åŒ–å’Œå‘é‡åº“ç®¡ç†
"""
import time
from typing import List, Dict, Any
import chromadb
from chromadb.config import Settings as ChromaSettings

from infrastructure.parser.ddl_parser import TableInfo, TableColumn
from infrastructure.logging.logger import get_logger

logger = get_logger("vector_service")


class VectorService:
    """å‘é‡åŒ–æœåŠ¡"""
    
    def __init__(self):
        """åˆå§‹åŒ–å‘é‡åº“ï¼ˆå†…å­˜æ¨¡å¼ï¼‰"""
        logger.info("Initializing Chroma vector store (in-memory mode)...")
        
        # åˆ›å»ºå†…å­˜å‘é‡åº“
        self.client = chromadb.Client(ChromaSettings(
            is_persistent=False,  # å†…å­˜æ¨¡å¼
            anonymized_telemetry=False
        ))
        
        # åˆ›å»ºé›†åˆï¼ˆç”¨äºå­˜å‚¨è¡¨ç»“æ„å‘é‡ï¼‰
        # ä½¿ç”¨é»˜è®¤çš„ embedding functionï¼ˆsentence-transformersï¼‰
        self.collection = self.client.get_or_create_collection(
            name="ddl_schema",
            metadata={"description": "Database schema embeddings"}
        )
        
        logger.info(f"Vector store initialized: collection=ddl_schema")
    
    def vectorize_tables(self, tables: List[TableInfo], file_id: str) -> int:
        """
        å‘é‡åŒ–è¡¨ç»“æ„ä¿¡æ¯
        
        Args:
            tables: è¡¨ç»“æ„ä¿¡æ¯åˆ—è¡¨
            file_id: æ–‡ä»¶ IDï¼ˆç”¨äºå…³è”ï¼‰
            
        Returns:
            int: å‘é‡åŒ–çš„æ¡ç›®æ•°é‡
        """
        logger.info(f"Starting vectorization for {len(tables)} tables...")
        start_time = time.time()
        
        # ğŸ”§ å…ˆåˆ é™¤è¯¥æ–‡ä»¶çš„æ—§å‘é‡ï¼ˆé¿å…é‡å¤ä¸Šä¼ å¯¼è‡´ ID å†²çªï¼‰
        self._delete_file_vectors(file_id)
        
        # ğŸ” æ£€æŸ¥å¹¶å»é‡è¡¨åï¼ˆä¿ç•™ç¬¬ä¸€ä¸ªå‡ºç°çš„è¡¨ï¼‰
        seen_names = set()
        unique_tables = []
        for table in tables:
            if table.name not in seen_names:
                unique_tables.append(table)
                seen_names.add(table.name)
            else:
                logger.warning(f"âš ï¸ Skipping duplicate table: {table.name}")
        
        if len(unique_tables) < len(tables):
            logger.warning(f"Removed {len(tables) - len(unique_tables)} duplicate tables")
            tables = unique_tables  # ä½¿ç”¨å»é‡åçš„è¡¨åˆ—è¡¨
        
        documents = []  # æ–‡æœ¬å†…å®¹
        metadatas = []  # å…ƒæ•°æ®
        ids = []        # å”¯ä¸€ ID
        
        for table in tables:
            # ä¸ºæ¯ä¸ªè¡¨ç”Ÿæˆå‘é‡
            table_doc = self._generate_table_document(table)
            table_id = f"{file_id}:table:{table.name}"
            
            documents.append(table_doc)
            metadatas.append({
                "type": "table",
                "file_id": file_id,
                "table_name": table.name,
                "column_count": len(table.columns)
            })
            ids.append(table_id)
            
            # ä¸ºæ¯ä¸ªå­—æ®µç”Ÿæˆå‘é‡
            for col in table.columns:
                col_doc = self._generate_column_document(table.name, col)
                col_id = f"{file_id}:column:{table.name}.{col.name}"
                
                documents.append(col_doc)
                metadatas.append({
                    "type": "column",
                    "file_id": file_id,
                    "table_name": table.name,
                    "column_name": col.name,
                    "data_type": col.data_type
                })
                ids.append(col_id)
        
        # æ‰¹é‡æ·»åŠ åˆ°å‘é‡åº“
        self.collection.add(
            documents=documents,
            metadatas=metadatas,
            ids=ids
        )
        
        elapsed = time.time() - start_time
        logger.info(
            f"Vectorization completed: {len(documents)} embeddings in {elapsed:.2f}s"
        )
        
        return len(documents)
    
    def _delete_file_vectors(self, file_id: str) -> None:
        """
        åˆ é™¤æŒ‡å®šæ–‡ä»¶çš„æ‰€æœ‰å‘é‡ï¼ˆé¿å…é‡å¤ä¸Šä¼ æ—¶ ID å†²çªï¼‰
        
        Args:
            file_id: æ–‡ä»¶ ID
        """
        try:
            # æŸ¥è¯¢è¯¥æ–‡ä»¶çš„æ‰€æœ‰å‘é‡ ID
            existing_results = self.collection.get(
                where={"file_id": file_id}
            )
            
            if existing_results and existing_results['ids']:
                existing_ids = existing_results['ids']
                logger.info(f"Deleting {len(existing_ids)} existing vectors for file_id={file_id}")
                
                # æ‰¹é‡åˆ é™¤
                self.collection.delete(ids=existing_ids)
                logger.info(f"Successfully deleted old vectors")
            else:
                logger.debug(f"No existing vectors found for file_id={file_id}")
                
        except Exception as e:
            logger.warning(f"Failed to delete old vectors: {e}")
            # ä¸é˜»å¡ä¸»æµç¨‹ï¼Œç»§ç»­æ·»åŠ æ–°å‘é‡
    
    def _generate_table_document(self, table: TableInfo) -> str:
        """
        ä¸ºè¡¨ç”Ÿæˆæ–‡æœ¬æè¿°ï¼ˆç”¨äºå‘é‡åŒ–ï¼‰
        
        Args:
            table: è¡¨ç»“æ„ä¿¡æ¯
            
        Returns:
            str: è¡¨çš„æ–‡æœ¬æè¿°
        """
        doc_parts = [
            f"è¡¨å: {table.name}",
            f"å­—æ®µæ•°é‡: {len(table.columns)}"
        ]
        
        # æ·»åŠ å­—æ®µåˆ—è¡¨
        col_names = [col.name for col in table.columns]
        doc_parts.append(f"å­—æ®µ: {', '.join(col_names)}")
        
        # æ·»åŠ ä¸»é”®
        if table.primary_keys:
            doc_parts.append(f"ä¸»é”®: {', '.join(table.primary_keys)}")
        
        # æ·»åŠ æ³¨é‡Š
        if table.comment:
            doc_parts.append(f"è¯´æ˜: {table.comment}")
        
        return " | ".join(doc_parts)
    
    def _generate_column_document(self, table_name: str, column: TableColumn) -> str:
        """
        ä¸ºå­—æ®µç”Ÿæˆæ–‡æœ¬æè¿°ï¼ˆç”¨äºå‘é‡åŒ–ï¼‰
        
        Args:
            table_name: è¡¨å
            column: å­—æ®µä¿¡æ¯
            
        Returns:
            str: å­—æ®µçš„æ–‡æœ¬æè¿°
        """
        doc_parts = [
            f"è¡¨: {table_name}",
            f"å­—æ®µ: {column.name}",
            f"ç±»å‹: {column.data_type}"
        ]
        
        # æ·»åŠ çº¦æŸ
        if column.constraints:
            doc_parts.append(f"çº¦æŸ: {', '.join(column.constraints)}")
        
        # æ·»åŠ æ³¨é‡Š
        if column.comment:
            doc_parts.append(f"è¯´æ˜: {column.comment}")
        
        return " | ".join(doc_parts)
    
    def query_schema(self, query_text: str, n_results: int = 5) -> List[Dict[str, Any]]:
        """
        æŸ¥è¯¢ç›¸å…³çš„è¡¨ç»“æ„ä¿¡æ¯
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            n_results: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            List[Dict[str, Any]]: ç›¸å…³çš„è¡¨ç»“æ„ä¿¡æ¯
        """
        results = self.collection.query(
            query_texts=[query_text],
            n_results=n_results
        )
        
        return results
    
    def get_stats(self) -> Dict[str, Any]:
        """
        è·å–å‘é‡åº“ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict[str, Any]: ç»Ÿè®¡ä¿¡æ¯
        """
        count = self.collection.count()
        return {
            "total_embeddings": count,
            "collection_name": self.collection.name,
            "status": "initialized" if count > 0 else "empty"
        }


# å…¨å±€å•ä¾‹
_vector_service_instance = None


def get_vector_service() -> VectorService:
    """
    è·å–å‘é‡æœåŠ¡å•ä¾‹
    
    Returns:
        VectorService: å‘é‡æœåŠ¡å®ä¾‹
    """
    global _vector_service_instance
    if _vector_service_instance is None:
        _vector_service_instance = VectorService()
    return _vector_service_instance
